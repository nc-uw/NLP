{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HntBnvSEbbLd"
   },
   "source": [
    "# DeepNLP 2 - Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wywD54KykkMQ"
   },
   "source": [
    "In this notebook, several Neural Language Models are implemented and analyzed.\n",
    "\n",
    "### **To get started, first run the following cell to create a PyDrive client and download data to your own Google Drive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30865,
     "status": "ok",
     "timestamp": 1552951074385,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "6kpDb3ncKOFN",
    "outputId": "f09c74ca-76cf-4e6e-f395-580a2f729803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K    1% |▎                               | 10kB 19.8MB/s eta 0:00:01\r",
      "\u001b[K    2% |▋                               | 20kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K    3% |█                               | 30kB 6.8MB/s eta 0:00:01\r",
      "\u001b[K    4% |█▎                              | 40kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K    5% |█▋                              | 51kB 5.3MB/s eta 0:00:01\r",
      "\u001b[K    6% |██                              | 61kB 6.3MB/s eta 0:00:01\r",
      "\u001b[K    7% |██▎                             | 71kB 7.0MB/s eta 0:00:01\r",
      "\u001b[K    8% |██▋                             | 81kB 7.9MB/s eta 0:00:01\r",
      "\u001b[K    9% |███                             | 92kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K    10% |███▎                            | 102kB 7.0MB/s eta 0:00:01\r",
      "\u001b[K    11% |███▋                            | 112kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K    12% |████                            | 122kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K    13% |████▎                           | 133kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K    14% |████▋                           | 143kB 16.5MB/s eta 0:00:01\r",
      "\u001b[K    15% |█████                           | 153kB 16.6MB/s eta 0:00:01\r",
      "\u001b[K    16% |█████▎                          | 163kB 16.5MB/s eta 0:00:01\r",
      "\u001b[K    17% |█████▋                          | 174kB 16.9MB/s eta 0:00:01\r",
      "\u001b[K    18% |██████                          | 184kB 17.1MB/s eta 0:00:01\r",
      "\u001b[K    19% |██████▎                         | 194kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K    20% |██████▋                         | 204kB 43.6MB/s eta 0:00:01\r",
      "\u001b[K    21% |███████                         | 215kB 20.5MB/s eta 0:00:01\r",
      "\u001b[K    22% |███████▎                        | 225kB 20.6MB/s eta 0:00:01\r",
      "\u001b[K    23% |███████▋                        | 235kB 20.7MB/s eta 0:00:01\r",
      "\u001b[K    24% |████████                        | 245kB 21.0MB/s eta 0:00:01\r",
      "\u001b[K    25% |████████▎                       | 256kB 21.0MB/s eta 0:00:01\r",
      "\u001b[K    26% |████████▋                       | 266kB 20.0MB/s eta 0:00:01\r",
      "\u001b[K    27% |█████████                       | 276kB 20.4MB/s eta 0:00:01\r",
      "\u001b[K    29% |█████████▎                      | 286kB 20.4MB/s eta 0:00:01\r",
      "\u001b[K    30% |█████████▋                      | 296kB 20.3MB/s eta 0:00:01\r",
      "\u001b[K    31% |██████████                      | 307kB 21.7MB/s eta 0:00:01\r",
      "\u001b[K    32% |██████████▎                     | 317kB 52.5MB/s eta 0:00:01\r",
      "\u001b[K    33% |██████████▋                     | 327kB 52.4MB/s eta 0:00:01\r",
      "\u001b[K    34% |███████████                     | 337kB 54.4MB/s eta 0:00:01\r",
      "\u001b[K    35% |███████████▎                    | 348kB 50.4MB/s eta 0:00:01\r",
      "\u001b[K    36% |███████████▋                    | 358kB 50.9MB/s eta 0:00:01\r",
      "\u001b[K    37% |████████████                    | 368kB 59.2MB/s eta 0:00:01\r",
      "\u001b[K    38% |████████████▎                   | 378kB 58.5MB/s eta 0:00:01\r",
      "\u001b[K    39% |████████████▋                   | 389kB 58.9MB/s eta 0:00:01\r",
      "\u001b[K    40% |█████████████                   | 399kB 27.3MB/s eta 0:00:01\r",
      "\u001b[K    41% |█████████████▎                  | 409kB 26.3MB/s eta 0:00:01\r",
      "\u001b[K    42% |█████████████▋                  | 419kB 26.3MB/s eta 0:00:01\r",
      "\u001b[K    43% |██████████████                  | 430kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K    44% |██████████████▎                 | 440kB 25.5MB/s eta 0:00:01\r",
      "\u001b[K    45% |██████████████▋                 | 450kB 25.9MB/s eta 0:00:01\r",
      "\u001b[K    46% |███████████████                 | 460kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K    47% |███████████████▎                | 471kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K    48% |███████████████▋                | 481kB 25.7MB/s eta 0:00:01\r",
      "\u001b[K    49% |████████████████                | 491kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K    50% |████████████████▎               | 501kB 51.9MB/s eta 0:00:01\r",
      "\u001b[K    51% |████████████████▋               | 512kB 51.5MB/s eta 0:00:01\r",
      "\u001b[K    52% |█████████████████               | 522kB 52.1MB/s eta 0:00:01\r",
      "\u001b[K    53% |█████████████████▎              | 532kB 54.5MB/s eta 0:00:01\r",
      "\u001b[K    54% |█████████████████▋              | 542kB 52.5MB/s eta 0:00:01\r",
      "\u001b[K    55% |██████████████████              | 552kB 56.2MB/s eta 0:00:01\r",
      "\u001b[K    57% |██████████████████▎             | 563kB 57.7MB/s eta 0:00:01\r",
      "\u001b[K    58% |██████████████████▋             | 573kB 57.9MB/s eta 0:00:01\r",
      "\u001b[K    59% |███████████████████             | 583kB 57.7MB/s eta 0:00:01\r",
      "\u001b[K    60% |███████████████████▎            | 593kB 57.5MB/s eta 0:00:01\r",
      "\u001b[K    61% |███████████████████▋            | 604kB 56.9MB/s eta 0:00:01\r",
      "\u001b[K    62% |████████████████████            | 614kB 62.5MB/s eta 0:00:01\r",
      "\u001b[K    63% |████████████████████▎           | 624kB 62.1MB/s eta 0:00:01\r",
      "\u001b[K    64% |████████████████████▋           | 634kB 63.8MB/s eta 0:00:01\r",
      "\u001b[K    65% |█████████████████████           | 645kB 65.9MB/s eta 0:00:01\r",
      "\u001b[K    66% |█████████████████████▎          | 655kB 64.7MB/s eta 0:00:01\r",
      "\u001b[K    67% |█████████████████████▋          | 665kB 47.5MB/s eta 0:00:01\r",
      "\u001b[K    68% |██████████████████████          | 675kB 46.5MB/s eta 0:00:01\r",
      "\u001b[K    69% |██████████████████████▎         | 686kB 46.2MB/s eta 0:00:01\r",
      "\u001b[K    70% |██████████████████████▋         | 696kB 46.4MB/s eta 0:00:01\r",
      "\u001b[K    71% |███████████████████████         | 706kB 46.3MB/s eta 0:00:01\r",
      "\u001b[K    72% |███████████████████████▎        | 716kB 46.4MB/s eta 0:00:01\r",
      "\u001b[K    73% |███████████████████████▋        | 727kB 46.7MB/s eta 0:00:01\r",
      "\u001b[K    74% |████████████████████████        | 737kB 46.2MB/s eta 0:00:01\r",
      "\u001b[K    75% |████████████████████████▎       | 747kB 45.6MB/s eta 0:00:01\r",
      "\u001b[K    76% |████████████████████████▋       | 757kB 44.9MB/s eta 0:00:01\r",
      "\u001b[K    77% |████████████████████████▉       | 768kB 60.4MB/s eta 0:00:01\r",
      "\u001b[K    78% |█████████████████████████▏      | 778kB 61.4MB/s eta 0:00:01\r",
      "\u001b[K    79% |█████████████████████████▌      | 788kB 61.9MB/s eta 0:00:01\r",
      "\u001b[K    80% |█████████████████████████▉      | 798kB 61.6MB/s eta 0:00:01\r",
      "\u001b[K    81% |██████████████████████████▏     | 808kB 60.4MB/s eta 0:00:01\r",
      "\u001b[K    82% |██████████████████████████▌     | 819kB 59.4MB/s eta 0:00:01\r",
      "\u001b[K    83% |██████████████████████████▉     | 829kB 59.7MB/s eta 0:00:01\r",
      "\u001b[K    85% |███████████████████████████▏    | 839kB 60.0MB/s eta 0:00:01\r",
      "\u001b[K    86% |███████████████████████████▌    | 849kB 62.0MB/s eta 0:00:01\r",
      "\u001b[K    87% |███████████████████████████▉    | 860kB 57.9MB/s eta 0:00:01\r",
      "\u001b[K    88% |████████████████████████████▏   | 870kB 56.2MB/s eta 0:00:01\r",
      "\u001b[K    89% |████████████████████████████▌   | 880kB 57.6MB/s eta 0:00:01\r",
      "\u001b[K    90% |████████████████████████████▉   | 890kB 57.9MB/s eta 0:00:01\r",
      "\u001b[K    91% |█████████████████████████████▏  | 901kB 58.0MB/s eta 0:00:01\r",
      "\u001b[K    92% |█████████████████████████████▌  | 911kB 60.0MB/s eta 0:00:01\r",
      "\u001b[K    93% |█████████████████████████████▉  | 921kB 58.5MB/s eta 0:00:01\r",
      "\u001b[K    94% |██████████████████████████████▏ | 931kB 58.3MB/s eta 0:00:01\r",
      "\u001b[K    95% |██████████████████████████████▌ | 942kB 58.9MB/s eta 0:00:01\r",
      "\u001b[K    96% |██████████████████████████████▉ | 952kB 58.3MB/s eta 0:00:01\r",
      "\u001b[K    97% |███████████████████████████████▏| 962kB 65.9MB/s eta 0:00:01\r",
      "\u001b[K    98% |███████████████████████████████▌| 972kB 68.1MB/s eta 0:00:01\r",
      "\u001b[K    99% |███████████████████████████████▉| 983kB 66.6MB/s eta 0:00:01\r",
      "\u001b[K    100% |████████████████████████████████| 993kB 22.0MB/s \n",
      "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hsuccess!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PENFoNBStA9K"
   },
   "source": [
    "... Now run the below cell to download all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8819,
     "status": "ok",
     "timestamp": 1552951432389,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "hPBlILmgtGQQ",
    "outputId": "ecdb43f6-4e0e-4d18-847e-fb677ad83b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Wikitext data downloaded!\n",
      "There are 28654 words in vocabulary\n",
      "Word id 0 stands for '<pad>'\n",
      "Word id 1 stands for '<unk>'\n",
      "Word id 2 stands for '<bos>'\n",
      "Word id 3 stands for '<eos>'\n",
      "Word id 4 stands for 'the'\n",
      "Word id 5 stands for ','\n",
      "Word id 6 stands for '.'\n",
      "Word id 7 stands for 'of'\n",
      "...\n",
      "tensor(1622368, device='cuda:0')\n",
      "Set up finished\n"
     ]
    }
   ],
   "source": [
    "import torch, pickle, os, sys, random, time\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if not os.path.isdir('./checkpoints'):\n",
    "  os.mkdir('./checkpoints')   # directory to save checkpoints\n",
    "\n",
    "# Download id2word\n",
    "f_wikitext = drive.CreateFile({'id': '1fBS7PyEOeQMuH5Ea1_hnEjU3PmFE7ZZc'})\n",
    "f_wikitext.GetContentFile('./wikitext.pkl') \n",
    "with open('./wikitext.pkl', 'rb') as f_in:\n",
    "  wikitext = pickle.load(f_in)\n",
    "\n",
    "wikitext['train'] = torch.LongTensor(wikitext['train']).cuda()\n",
    "wikitext['dev'] = torch.LongTensor(wikitext['valid']).cuda()\n",
    "wikitext['test'] = torch.LongTensor(wikitext['test']).cuda()\n",
    "idx_to_word = wikitext['id2word']\n",
    "\n",
    "print(\"Wikitext data downloaded!\")\n",
    "# Demonstrate id2word\n",
    "print('There are ' + str(len(idx_to_word)) + ' words in vocabulary')\n",
    "for id in range(8):\n",
    "  print('Word id ' + str(id) + \" stands for '\" + str(idx_to_word[id]) + \"\\'\")\n",
    "print('...')\n",
    "print((wikitext['train'] > 0).sum())\n",
    "    \n",
    "print('Set up finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoRX8GIubwBf"
   },
   "source": [
    "### Vanilla neural language models: \n",
    "\n",
    "The input to the model is a *minibatch* of sequences which takes the form of a  $N \\times L$ matrix  where $N$ is the batch size and $L$ is the maximum sequence length. For each minibatch, the models produce an $N \\times L \\times V$ tensor where $V$ is the size of the vocabulary. This tensor stores a prediction of the next word for every position of every sequence in the batch. Note that each batch is padded to dimensionality $L=40$ using the special padding token <*pad>*; similarly, each sequence begins with the <*bos>* token and ends with the <*eos>* token.\n",
    "\n",
    "To get familiar with the inputs and outputs, let's first take a look at a simple bigram language model. The Language Models are trained on the WikiText2 dataset which as ~2 million tokens. The *perplexity* metric is used to evaluate the models on the dev set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 367462,
     "status": "ok",
     "timestamp": 1552957605719,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "-FdRIiSi-lKT",
    "outputId": "ec7d28a0-907b-4478-ee70-df3e7ee5dc83"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7249d8ffe94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# A simple model that predicts the next word given just the previous word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBigramLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBigramLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# A simple model that predicts the next word given just the previous word\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(BigramLM, self).__init__()\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.W = nn.Linear(self.d_emb, self.vocab_size) # output matrix\n",
    "        \n",
    "\n",
    "    def forward(self, batch): \n",
    "        # each example in a batch is of the form <BOS> w1 w2 ... wn <EOS>\n",
    "        bsz, seq_len = batch.size()\n",
    "        embs = self.embeddings(batch)\n",
    "        #print(embs.size())\n",
    "        logits = self.W(embs)\n",
    "        return logits\n",
    "      \n",
    "# function to evaluate LM perplexity on some input data\n",
    "def compute_perplexity(dataset, net, bsz=64):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    num_examples, seq_len = dataset.size()\n",
    "    \n",
    "\n",
    "    batches = [(start, start + bsz) for start in\\\n",
    "               range(0, num_examples, bsz)]\n",
    "   \n",
    "    total_unmasked_tokens = 0. # init count of unpadded tokens\n",
    "    nll = 0.\n",
    "    for b_idx, (start, end) in enumerate(batches):\n",
    "      \n",
    "        batch = dataset[start:end]\n",
    "        ut = torch.nonzero(batch).size(0)\n",
    "        preds = net(batch)\n",
    "        targets = batch[:, 1:].contiguous().view(-1)\n",
    "        preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "        loss = criterion(preds, targets)\n",
    "        nll += loss.detach()\n",
    "        total_unmasked_tokens += ut\n",
    "\n",
    "    perplexity = torch.exp(nll / total_unmasked_tokens).cpu()\n",
    "    return perplexity.data\n",
    "    \n",
    "\n",
    "# training loop for language models\n",
    "def train_lm(dataset, params, net):\n",
    "    \n",
    "    # computing the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    num_examples, seq_len = dataset.size()  \n",
    "    batches = [(start, start + params['batch_size']) for start in\\\n",
    "               range(0, num_examples, params['batch_size'])]\n",
    "    #print(batches)\n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        random.shuffle(batches)\n",
    "        # for each batch, calculate loss and optimize model parameters            \n",
    "        for b_idx, (start, end) in enumerate(batches):\n",
    "            \n",
    "            batch = dataset[start:end]\n",
    "            preds = net(batch)\n",
    "    \n",
    "            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss += loss\n",
    "\n",
    "        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev perplexity: %0.2f' %\\\n",
    "              (epoch, ep_loss, time.time()-start_time, compute_perplexity(wikitext['dev'], net)))\n",
    "\n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 50\n",
    "params['batch_size'] = 128\n",
    "params['epochs'] = 5\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "bigramNet = BigramLM(params)\n",
    "bigramNet.cuda()\n",
    "train_lm(wikitext['train'], params, bigramNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2673,
     "status": "ok",
     "timestamp": 1552885126049,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "g2foNsYqV04X",
    "outputId": "e55b5ba5-ad03-452e-ea4f-7e4e59ad649b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BigramLM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(bigramNet, 'Bigram.pkl') # save on colab machine\n",
    "model_file = drive.CreateFile()\n",
    "model_file.SetContentFile('Bigram.pkl') # copy the file from colab machine to google drive\n",
    "model_file.Upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14yaupJdP394"
   },
   "source": [
    "#### Perplexity\n",
    "\n",
    "Run the following cell to compute the perplexity on the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33454,
     "status": "ok",
     "timestamp": 1552885162641,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "lGt7JeMao2Oq",
    "outputId": "8c8b24e5-f845-4cd4-8177-6f1871449644"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train perplexity: 249.88\n",
      "dev perplexity: 259.63\n",
      "test perplexity: 242.17\n"
     ]
    }
   ],
   "source": [
    "bigramNet.eval() # evaluation of the network\n",
    "print('%s perplexity: %0.2f' % ('train', compute_perplexity(wikitext['train'], bigramNet)))\n",
    "print('%s perplexity: %0.2f' % ('dev', compute_perplexity(wikitext['dev'], bigramNet)))\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], bigramNet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXQv3RADq6Tf"
   },
   "source": [
    "### Recurrent neural language models: \n",
    "In this part, a recurrent neural language model is implemented. Run the following cell to evaluate the RNNLM -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251281,
     "status": "ok",
     "timestamp": 1552886347508,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "Uz4tfUfJrsGk",
    "outputId": "2115f3e2-78be-4a43-ed50-34a6d5776254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 7927.28, time: 226.63 sec, dev perplexity: 151.22\n",
      "epoch: 1, loss: 6799.85, time: 226.52 sec, dev perplexity: 127.21\n",
      "epoch: 2, loss: 6135.33, time: 226.65 sec, dev perplexity: 123.18\n",
      "epoch: 3, loss: 5587.93, time: 226.62 sec, dev perplexity: 126.27\n",
      "epoch: 4, loss: 5117.60, time: 226.62 sec, dev perplexity: 135.96\n"
     ]
    }
   ],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb'] #input_size\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.d_hid = params['d_hid'] #hidden_size\n",
    "        self.RNNLM = nn.LSTM(self.d_emb, self.d_hid, batch_first = True)\n",
    "        self.W = nn.Linear(self.d_hid, self.vocab_size) # output matrix\n",
    "    \n",
    "    def init_hidden(self,bsz):\n",
    "        hx = torch.randn(1, bsz, self.d_hid).cuda()\n",
    "        cx = torch.randn(1, bsz, self.d_hid).cuda()\n",
    "        return (hx, cx)\n",
    "              \n",
    "    def forward(self, batch):\n",
    "        bsz, seq_len = batch.size()\n",
    "        embs = self.embeddings(batch)\n",
    "        (hx, cx) = self.init_hidden(bsz)   \n",
    "        '''\n",
    "        print ('...printing embs.size() = ', embs.size())\n",
    "        print ('...printing hx.size() = ', hx.size())\n",
    "        print ('...printing cx.size() = ', cx.size())\n",
    "        '''\n",
    "        out, (hx, cx) = self.RNNLM(embs, (hx, cx))\n",
    "           \n",
    "        logits = self.W(out)\n",
    "        return logits \n",
    "       \n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 512\n",
    "params['d_hid'] = 512\n",
    "params['batch_size'] = 50\n",
    "params['epochs'] = 5\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "RNNnet = RNNLM(params)\n",
    "RNNnet.cuda()\n",
    "train_lm(wikitext['train'], params, RNNnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 84995,
     "status": "ok",
     "timestamp": 1552886432496,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "UmhoWt1J_pqT",
    "outputId": "b22f5dc2-c44a-41b9-9be5-ba1707c8b61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train perplexity: 24.52\n",
      "dev perplexity: 135.97\n",
      "test perplexity: 127.26\n"
     ]
    }
   ],
   "source": [
    "RNNnet.eval() # evaluate the network\n",
    "print('%s perplexity: %0.2f' % ('train', compute_perplexity(wikitext['train'], RNNnet)))\n",
    "print('%s perplexity: %0.2f' % ('dev', compute_perplexity(wikitext['dev'], RNNnet)))\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], RNNnet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1552758521677,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 420
    },
    "id": "Vpznt32uWFux",
    "outputId": "66f4510a-1223-495d-b0e1-3f2b02e69c74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNLM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# saving\n",
    "torch.save(RNNnet, 'LSTM.pkl') #save on colab machine"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of cs690d_hw2.ipynb",
   "provenance": [
    {
     "file_id": "162oOo5-_xHZfCWj917cGvkSJeoar7eWv",
     "timestamp": 1552432956964
    },
    {
     "file_id": "1AYqmQ6bQWVLNbFi4PRif5aljeTNi_zqj",
     "timestamp": 1551289091714
    },
    {
     "file_id": "13RjLwplbdgrkgaRVdIv9l3Hro8ByC0bM",
     "timestamp": 1551043687775
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
