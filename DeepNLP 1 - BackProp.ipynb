{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LsFKULpuoTz"
   },
   "source": [
    "## Deep NLP  1 - BackProp for NNs, DANs, RNNs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wQ0Tt4UvhdF"
   },
   "source": [
    "### PART A: IMPLEMENTING BACKPROP\n",
    "\n",
    "In this notebook *backpropagation*  is implemented to train several different neural network architectures. Specifically, the partial derivatives of the provided loss function is computed with respect to each parameter of the network. This is then checked automatically against the output of pytorch's autograd.\n",
    "\n",
    "**Run the below cell to import pytorch and set up the gradient checking functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4Q2X-BB7YZk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# checks equality between gradients and those from autograd\n",
    "# 'params' and 'gradient' are both dictionaries with the same keys, the names of parameters.\n",
    "# 'params' contains model parameters augmented with pytorch's automatically-computed gradients.  \n",
    "# 'gradient' contains tensors you calculated yourself.  This function assumes pytorch computed\n",
    "# them correctly, and checks whether your calculations match.\n",
    "\n",
    "def gradient_check(params, gradient):\n",
    "    all_good = True\n",
    "    for key in params.keys():\n",
    "        if params[key].grad.size() != gradient[key].size():\n",
    "            print('GRADIENT ERROR for parameter %s, SIZE ERROR\\nyour size: %s\\nactual size: %s\\n'\\\n",
    "                % (key, gradient[key].size(), \n",
    "                   params[key].grad.size()))\n",
    "            all_good = False\n",
    "        elif not torch.allclose(params[key].grad, gradient[key], atol=1e-6):\n",
    "            print('GRADIENT ERROR for parameter %s, VALUE ERROR\\nyours: %s\\nactual: %s\\n'\\\n",
    "                % (key, gradient[key].detach(), \n",
    "                   params[key].grad))\n",
    "            all_good = False\n",
    "            \n",
    "    return all_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxCa23c_7asB"
   },
   "source": [
    "\n",
    "### A.1: Single neurons \n",
    "The below cell trains a network with just single neurons in each layer on a small dataset of ten examples. The network is defined as:\n",
    "\n",
    "<center>$\\text{h} = \\tanh(w_1 * \\text{input})$</center>\n",
    "\n",
    "<center>$\\text{pred} = \\tanh(w_2 * \\text{h})$</center>\n",
    "\n",
    "Once the partial derivatives $\\frac{\\partial{L}}{\\partial{w_1}}$ and $\\frac{\\partial{L}}{\\partial{w_2}}$ sre implemented correctly, \"SUCCESS\" message is displayed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1550637158134,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 300
    },
    "id": "V6c-2vlxut1a",
    "outputId": "d43274a7-8b47-460b-f24b-6122f97f3191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! you passed the gradient check.\n"
     ]
    }
   ],
   "source": [
    "# initialize model parameters\n",
    "params = {}\n",
    "params['w1'] = torch.randn(1, 1, requires_grad=True) # input > hidden with scalar weight w1\n",
    "params['w2'] = torch.randn(1, 1, requires_grad=True) # hidden > output with scalar weight w2\n",
    "\n",
    "# set up some training data\n",
    "inputs = torch.randn(20, 1)\n",
    "targets = inputs / 2\n",
    "\n",
    "# training loop\n",
    "all_good = True\n",
    "for i in range(len(inputs)):\n",
    "    \n",
    "    ## forward prop, then compute loss.\n",
    "    a = params['w1'] * inputs[i] # intermediate variable, following lecture notes\n",
    "    hidden = torch.tanh(a)\n",
    "    b = params['w2'] * hidden\n",
    "    pred = torch.tanh(b)\n",
    "    loss = 0.5 * (targets[i] - pred) ** 2 # compute square loss\n",
    "    loss.backward() # runs autograd\n",
    "    \n",
    "    ####################\n",
    "    # IMPLEMENT BACKPROP: START\n",
    "    manual_gradient = {}\n",
    "    manual_gradient['w1'] = torch.zeros(params['w1'].size()) # implement dL/dw1\n",
    "    manual_gradient['w2'] = torch.zeros(params['w2'].size()) # implement dL/dw2\n",
    "    \n",
    "    #Intermediate Computation for w2\n",
    "    dldo = -1*(targets[i] - pred)\n",
    "    doda = 1 - pred*pred\n",
    "    dadw = hidden\n",
    "    manual_gradient['w2'] = dldo*doda*dadw\n",
    "    \n",
    "    #Intermediate Computation for w1\n",
    "    dadh = params['w2']\n",
    "    dhdb = (1- hidden*hidden)\n",
    "    dbdw = inputs[i]\n",
    "    manual_gradient['w1'] = dldo*doda*dadh*dhdb*dbdw \n",
    "   \n",
    "    # IMPLEMENT BACKPROP: STOP\n",
    "    ####################\n",
    "\n",
    "    if not gradient_check(params, manual_gradient):\n",
    "        all_good = False\n",
    "        break\n",
    "    \n",
    "    # zero gradients after each training example\n",
    "    params['w1'].grad.zero_()\n",
    "    params['w2'].grad.zero_() \n",
    "    \n",
    "if all_good:\n",
    "    print('SUCCESS! you passed the gradient check.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqlJWq5sf1al"
   },
   "source": [
    "### A.2: Deep averaging network (DAN) for **SENTIMENT ANALYSIS**\n",
    "DAN is a more complex network featuring multiple matrix/vector operations. Instead of taking single numbers as input, this network takes in *a single sequence of word embeddings* associated with a sentence. Let's call the input $X$; it has dimensionality $N \\times D$, where $N$ is the number of words in the sentence and $D$ is the word embedding dimensionality. We'll denote the $i^{th}$ word embedding in the sentence, which corresponds to the $i^{th}$ row of $X$, as $X_i$. The network is trained using softmax / cross entropy loss for **SENTIMENT ANALYSIS**, so each input is associated with a target value $t$ (positive, negative, or neutral). The network is described by the following set of equations:\n",
    "\n",
    "<center>$\\text{ave} = \\frac{1}{N} \\sum_{i=0}^{N} X_i$</center>\n",
    "\n",
    "<center>$\\text{h} = \\text{ReLu}(W_1 \\cdot \\text{ave})$</center>\n",
    "\n",
    "<center>$\\text{pred} = \\text{softmax}(W_2 \\cdot h)$</center>\n",
    "\n",
    "where $\\text{ReLu}(x) = max(0, x)$. The defined $\\text{softmax}$ derivative is used to compute three partial derivatives: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, and $\\frac{\\partial{L}}{\\partial{X}}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1550637160399,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 300
    },
    "id": "Z7YbZFwMwsgA",
    "outputId": "342d62a9-a0b1-47a6-9a04-a3243b2a1ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! you passed the gradient check.\n"
     ]
    }
   ],
   "source": [
    "# Setting HyperParams\n",
    "N = 4 # all sentences will be of length 4\n",
    "D = 5 # word embedding dim = 5\n",
    "M = 3 # hidden dimensionality\n",
    "labels = {'negative':0, 'neutral':1, 'positive':2}\n",
    "vocab = {'really':0, 'movie':1, 'was':2, 'good':3, 'not':4, 'okay':5}\n",
    "num_labels = len(labels)\n",
    "len_vocab = len(vocab)\n",
    "\n",
    "# initialize model parameters\n",
    "params = {}\n",
    "params['X'] = torch.randn(len_vocab, D, requires_grad=True)\n",
    "params['W1'] = torch.randn(M, D, requires_grad=True) \n",
    "params['W2'] = torch.randn(num_labels, M, requires_grad=True) \n",
    "\n",
    "# set up some training data\n",
    "inputs = [('positive', 'movie was really good'),\n",
    "          ('neutral', 'really really really okay'),\n",
    "          ('negative', 'movie was not good')]\n",
    "\n",
    "# training loop\n",
    "all_good = True\n",
    "for i in range(len(inputs)):\n",
    "    \n",
    "    # obtain word embeddings for input sentence\n",
    "    target, sentence = inputs[i]\n",
    "    target = labels[target]\n",
    "    input = torch.LongTensor(N)\n",
    "    for j, w in enumerate(sentence.split()):\n",
    "        input[j] = vocab[w]\n",
    "    \n",
    "    ## forward prop, then compute loss.\n",
    "    ave = torch.zeros(D) # Compute the word embedding average\n",
    "    davedx = torch.zeros(len_vocab, D)\n",
    "    for j in range(N):\n",
    "        w_idx = input[j]\n",
    "        ave += params['X'][w_idx]\n",
    "        davedx[w_idx] = 1.0\n",
    "    ave = ave / N\n",
    "    \n",
    "    # pass it through DAN\n",
    "    a = torch.mv(params['W1'], ave)\n",
    "    h = torch.relu(a)\n",
    "    b = torch.mv(params['W2'], h)\n",
    "    pred = torch.softmax(b, 0)\n",
    "    \n",
    "    loss = -1 * torch.log(pred[target]) # negative log likelihood of target class\n",
    "    loss.backward() # runs autograd\n",
    "    \n",
    "    # Defined derivative for the softmax / CE loss as \"dLdb\"\n",
    "    dLdb = pred.clone().detach()\n",
    "    dLdb[target] -= 1.\n",
    "    \n",
    "    #print(ave.size())\n",
    "    #print(b.size())\n",
    "    #print(h.size())\n",
    "    #print(dLdb.size())\n",
    "    #print(a.size())\n",
    "    \n",
    "    ####################\n",
    "    # IMPLEMENT BACKPROP: START\n",
    "    your_gradient = {}\n",
    "    your_gradient['W2'] = torch.zeros(params['W2'].size()) # implement dL/dW2\n",
    "    your_gradient['W1'] = torch.zeros(params['W1'].size()) # implement dL/dW1\n",
    "    your_gradient['X'] = torch.zeros(params['X'].size()) # implement dL/dX\n",
    "    \n",
    "    your_gradient['W2'] =  torch.ger(dLdb,h)\n",
    "    \n",
    "    #Intermediate\n",
    "    dbdh = params['W2']\n",
    "    dhda = a > 0\n",
    "    dadw = ave\n",
    "  \n",
    "    prod1 = torch.matmul(dLdb,dbdh)\n",
    "    prod2 = dhda.float()*prod1\n",
    "    your_gradient['W1'] = torch.ger(prod2,dadw)\n",
    "    \n",
    "    prod3 = torch.mm(dLdb.unsqueeze(0), params[\"W2\"])\n",
    "    prod4 = prod3 * (a>0).float()\n",
    "    grad_temp = (1/N) * torch.mm(prod4 ,params['W1'])\n",
    "\n",
    "    for j in range(N):\n",
    "      w_idx = input[j]\n",
    "      your_gradient['X'][w_idx] = your_gradient['X'][w_idx]+grad_temp\n",
    "\n",
    "    # IMPLEMENT BACKPROP: STOP\n",
    "    ####################\n",
    "    \n",
    "    if not gradient_check(params, your_gradient):\n",
    "        all_good = False\n",
    "        break\n",
    "    \n",
    "    # zero gradients after each training example\n",
    "    params['X'].grad.zero_()\n",
    "    params['W1'].grad.zero_()\n",
    "    params['W2'].grad.zero_() \n",
    "    \n",
    "if all_good:\n",
    "    print('SUCCESS! you passed the gradient check.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0IEmz-08F-O"
   },
   "source": [
    "### A.3: Recurrent neural network (RNN) for **SENTIMENT ANALYSIS**\n",
    "This implements *backpropagation through time*,  an extension of backpropagation for RNNs, for **SENTIMENT ANALYSIS**. The network architecture is a variant of an RNN with a multiplicative term. The computations in an RNN proceed sequentially, so for inputs where $N=4$, we compute four different hidden states ($h_0, h_1, h_2, h_3$) and feed the final hidden state $h_3$ to the output layer. The network is then defined as:\n",
    "\n",
    "<center>$h_i= \\tanh(W_h \\cdot h_{i-1} + W_x \\cdot X_i + h_{i-1} * h_{i-2}$)</center>\n",
    "<center>$\\text{pred} = \\text{softmax}(W_\\text{out} \\cdot h_3)$</center>\n",
    "\n",
    "##### It is helpful to think of an RNN as a feed-forward network by \"unrolling\" it over the time dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1550637163022,
     "user": {
      "displayName": "Neha Choudhary",
      "photoUrl": "https://lh5.googleusercontent.com/-LgO3bBUJ1yY/AAAAAAAAAAI/AAAAAAAARUs/aWjcPlw28ZM/s64/photo.jpg",
      "userId": "11210265198609739465"
     },
     "user_tz": 300
    },
    "id": "C8JTEmr--TJV",
    "outputId": "f961f035-00b1-422e-d691-e69576548a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! you passed the gradient check.\n"
     ]
    }
   ],
   "source": [
    "# initialize model parameters\n",
    "params = {}\n",
    "params['X'] = torch.randn(len_vocab, D, requires_grad=True)\n",
    "params['Wx'] = torch.randn(M, D, requires_grad=True) \n",
    "params['Wh'] = torch.randn(M, M, requires_grad=True)\n",
    "params['Wout'] = torch.randn(num_labels, M, requires_grad=True) \n",
    "\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "# training loop\n",
    "all_good = True\n",
    "for i in range(len(inputs)):\n",
    "    \n",
    "    # obtain word embeddings for input sentence\n",
    "    target, sentence = inputs[i]\n",
    "    target = labels[target]\n",
    "    input = torch.LongTensor(N)\n",
    "    for j, w in enumerate(sentence.split()):\n",
    "        input[j] = vocab[w]\n",
    "    \n",
    "    ## forward prop, then compute loss.\n",
    "    hiddens = {} # stores hidden state / intermediate vars at each timestep\n",
    "    for j in range(N):\n",
    "        w_idx = input[j]\n",
    "        hiddens[j] = {}\n",
    "        \n",
    "        # no previous hidden state, just project word embedding\n",
    "        if j == 0:\n",
    "            hiddens[j]['a'] = torch.mv(params['Wx'], params['X'][w_idx])\n",
    "            hiddens[j]['a'].retain_grad() #Dont forget to comment\n",
    "            \n",
    "        elif j == 1:\n",
    "            hiddens[j]['a'] = torch.mv(params['Wx'], params['X'][w_idx]) + \\\n",
    "                torch.mv(params['Wh'], hiddens[j-1]['h'])\n",
    "            hiddens[j]['a'].retain_grad() #Dont forget to comment\n",
    "            \n",
    "        else:\n",
    "            hiddens[j]['a'] = torch.mv(params['Wx'], params['X'][w_idx]) + \\\n",
    "                torch.mv(params['Wh'], hiddens[j-1]['h']) + \\\n",
    "                hiddens[j-1]['h'] * hiddens[j-2]['h']\n",
    "            hiddens[j]['a'].retain_grad() #Dont forget to comment\n",
    "            \n",
    "        hiddens[j]['h'] = torch.tanh(hiddens[j]['a'])\n",
    "        hiddens[j]['h'].retain_grad()\n",
    "\n",
    "    \n",
    "    b = torch.mv(params['Wout'], hiddens[N-1]['h'])\n",
    "    pred = torch.softmax(b, 0)\n",
    "    \n",
    "    loss = -1 * torch.log(pred[target]) # negative log likelihood of target class\n",
    "    loss.backward() # runs autograd\n",
    "    \n",
    "    # Derivative for the softmax / CE loss as \"dLdb\"\n",
    "    dLdb = pred.clone().detach()\n",
    "    dLdb[target] -= 1.\n",
    "    \n",
    "    ####################\n",
    "    # IMPLEMENT BACKPROP: START\n",
    "    your_gradient = {}\n",
    "    your_gradient['Wout'] = torch.zeros(params['Wout'].size()) # implement dL/dWout\n",
    "    your_gradient['X'] = torch.zeros(params['X'].size()) # implement dL/dX\n",
    "    your_gradient['Wx'] = torch.zeros(params['Wx'].size()) # implement dL/dWx\n",
    "    your_gradient['Wh'] = torch.zeros(params['Wh'].size()) # implement dL/dWh\n",
    "    \n",
    "    #your_gradient['Wout'] \n",
    "    #print(dLdb.size())\n",
    "    #print(hiddens[N-1]['h'].size() )\n",
    "    #print(params['X'][w_idx].size())\n",
    "    \n",
    "    your_gradient['Wout'] = torch.ger(dLdb,hiddens[N-1]['h'])\n",
    "    dLdhs = torch.zeros((N, M))\n",
    "    \n",
    "    for j in range(N - 1, -1, -1):\n",
    "      if j == N-1:\n",
    "        dLdhs[j] = torch.mv(params['Wout'].t(), dLdb) \n",
    "      #print(dLdhs[j])\n",
    "      w_idx = input[j]\n",
    "      dhsdas = (1 - (torch.tanh(hiddens[j]['a'])** 2)) * dLdhs[j]\n",
    "   \n",
    "      your_gradient['X'][w_idx] = your_gradient['X'][w_idx] + torch.mv(params['Wx'].t(), dhsdas)\n",
    "      your_gradient['Wx'] = your_gradient['Wx']+ torch.ger(dhsdas, params['X'][w_idx])\n",
    "      if j > 0:\n",
    "        your_gradient['Wh'] = your_gradient['Wh'] + torch.ger(dhsdas, hiddens[j - 1]['h'])\n",
    "        dLdhs[j - 1] = dLdhs[j - 1] + torch.mv(params['Wh'].t(), dhsdas)\n",
    "        if j > 1:\n",
    "          dLdhs[j - 2] = dLdhs[j - 2] + hiddens[j - 1]['h'] * dhsdas\n",
    "          dLdhs[j - 1] = dLdhs[j - 1] + hiddens[j - 2]['h'] * dhsdas\n",
    "   \n",
    "    # IMPLEMENT BACKPROP: STOP\n",
    "    #############################################\n",
    "            \n",
    "    if not gradient_check(params, your_gradient):\n",
    "        all_good = False\n",
    "        break\n",
    "    \n",
    "    # zero gradients after each training example\n",
    "    params['X'].grad.zero_()\n",
    "    params['Wx'].grad.zero_()\n",
    "    params['Wh'].grad.zero_() \n",
    "    params['Wout'].grad.zero_() \n",
    "\n",
    "    \n",
    "if all_good:\n",
    "    print('SUCCESS! you passed the gradient check.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fcphJoTHtx4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of hw1.ipynb",
   "provenance": [
    {
     "file_id": "1ouK0POKEYu8RiCiv-2mcs45Zxzdfbp0D",
     "timestamp": 1549852571739
    },
    {
     "file_id": "1Hg0Ekz-_7jQ_4w2IQxg8RS6NBXRcUgsK",
     "timestamp": 1549393569449
    },
    {
     "file_id": "1GFgM_ZnrQryCQSzBpKEVyXiIMwv5UIip",
     "timestamp": 1549173722336
    },
    {
     "file_id": "1-FMuvqRKe-RpSFrSKcim90L7emdss8Er",
     "timestamp": 1549173709286
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
